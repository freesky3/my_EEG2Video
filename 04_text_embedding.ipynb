{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5020899",
   "metadata": {},
   "source": [
    "This file used to generate text embeddings using pre-trained \"openai/clip-vit-large-patch14\" model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0080e217",
   "metadata": {},
   "source": [
    "## CONFIGURATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d97b9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"model_name\": \"openai/clip-vit-large-patch14\",\n",
    "    \"text_path\": \"data\\\\metadata\\\\video_descriptions_en_short.json\", \n",
    "    \"save_path\": \"D:\\\\sjtu文件夹\\\\PLUS课程文件夹\\\\PRP\\\\my_EEG2Video\\\\data\\\\metadata\\\\text_embedding.pt\", \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cd4e2c",
   "metadata": {},
   "source": [
    "## Load tokenizer and text embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a545f43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\app\\miniconda\\envs\\eeg_test\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: {device}\n",
      "Loading tokenizer...\n",
      "Loading text encoder...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPTextModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"[INFO] Using device: {device}\")\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(CONFIG[\"model_name\"])\n",
    "\n",
    "print(\"Loading text encoder...\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(CONFIG[\"model_name\"]).to(device)\n",
    "text_encoder.eval() # 设置为评估模式\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ec05d6",
   "metadata": {},
   "source": [
    "## Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3603d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "prompts_dict = json.load(open(CONFIG[\"text_path\"], \"r\"))\n",
    "prompts = []\n",
    "for i in range(len(prompts_dict)):\n",
    "    prompts.append(prompts_dict[f\"{i+1}.mp4\"])\n",
    "\n",
    "text_inputs = tokenizer(\n",
    "        prompts,\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length, # 通常是 77\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "input_ids = text_inputs.input_ids.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af936c7e",
   "metadata": {},
   "source": [
    "## Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8dadac67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings shape: torch.Size([250, 77, 768])\n",
      "Successfully saved text embeddings to: save_path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(input_ids)[0] # 输出的元组中，第一个元素是 last_hidden_state\n",
    "\n",
    "text_embeddings = text_embeddings.cpu()\n",
    "\n",
    "# 打印形状以供验证\n",
    "# 形状应该是 (N, 77, 768)，其中 N 是提示的数量，77 是 token 数量，768 是嵌入维度\n",
    "print(f\"Generated embeddings shape: {text_embeddings.shape}\")\n",
    "\n",
    "# 4. 保存嵌入向量\n",
    "save_path = CONFIG[\"save_path\"]\n",
    "output_path = os.path.join(save_path)\n",
    "torch.save(text_embeddings, output_path)\n",
    "\n",
    "print(f\"Successfully saved text embeddings to: save_path\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
